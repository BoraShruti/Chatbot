{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoraShruti/Chatbot/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dabf4d2-d1e6-4253-9260-53b6e86278bb",
      "metadata": {
        "id": "2dabf4d2-d1e6-4253-9260-53b6e86278bb"
      },
      "source": [
        "# Chatbot Using Torchscript, Attention & Profiling\n",
        "\n",
        "This notebook demonstrates how to build a sequence-to-sequence chatbot using PyTorch. The chatbot uses an encoderâ€“decoder architecture with Luong attention and includes data preprocessing, vocabulary construction, model definition, training routines (with teacher forcing), evaluation functions, and checkpoint saving/loading.\n",
        "\n",
        "The notebook is highly commented so that each part of the code is explained in detail.\n",
        "\n",
        "Key technologies and concepts include:\n",
        "- **PyTorch** for deep learning model construction and GPU/CPU computation.\n",
        "- **GRU-based Encoder/Decoder** for sequence modeling.\n",
        "- **Luong Attention** for weighted context during decoding.\n",
        "- **Data preprocessing**: normalization, tokenization, padding, and masking.\n",
        "- **Checkpointing** to save and resume model training.\n",
        "- **Torch profiler** for performance insights (using `torch.profiler`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8d5c2da-bd4c-43fa-bf55-dd0a8db72e8f",
      "metadata": {
        "id": "e8d5c2da-bd4c-43fa-bf55-dd0a8db72e8f"
      },
      "source": [
        "## Section 1: Imports, Device Configuration, and Corpus Setup\n",
        "\n",
        "> **Text Box:**\n",
        "> This section loads all required libraries, sets up the computing device (GPU if available, otherwise CPU), and downloads the corpus using Convokit.\n",
        "> Additional inline comments explain each import and the purpose of key lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "060b8a66-5b9c-4c74-8d98-9ad53f2cb615",
      "metadata": {
        "id": "060b8a66-5b9c-4c74-8d98-9ad53f2cb615"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries for deep learning, data processing, and file handling\n",
        "import torch                 # Main PyTorch library for tensor operations and GPU acceleration\n",
        "import torch.nn as nn        # Module for building neural networks\n",
        "import torch.optim as optim  # Contains optimization algorithms like SGD and Adam\n",
        "import torch.nn.functional as F  # Provides functions like softmax and loss functions\n",
        "import csv                   # For CSV file read/write operations\n",
        "import random                # Standard library for random number generation\n",
        "import re                    # Regular expressions for string matching and manipulation\n",
        "import os                    # Operating system interfaces for file system operations\n",
        "import unicodedata           # For Unicode text normalization\n",
        "import codecs                # For encoding and decoding operations\n",
        "import json                  # To work with JSON files\n",
        "import itertools             # Used for efficient looping operations, especially for batching\n",
        "import math                  # Provides access to mathematical functions\n",
        "from io import open          # Import open from io for file operations with encoding support\n",
        "from torch.profiler import profile, record_function, ProfilerActivity  # Profiling tools to analyze performance\n",
        "\n",
        "# Enables inline plotting for visualizations within the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Select the computing device: if an accelerator (like a GPU) is available, use it; else, fall back to CPU\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Print a message signaling that corpus setup is beginning\n",
        "print(\"Setting up corpus...\")\n",
        "\n",
        "# Install the Convokit library which is used to download and manage conversation corpora\n",
        "!pip install convokit\n",
        "\n",
        "# Import necessary components from the convokit library\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# Define a custom directory where the corpus data will be stored\n",
        "custom_path = \"/content/drive/MyDrive/Data_cornell\"\n",
        "\n",
        "# Download the 'movie-corpus' to the specified custom path\n",
        "corpus = Corpus(filename=download(\"movie-corpus\", data_dir=custom_path))\n",
        "\n",
        "# Confirm the corpus has been downloaded by printing the path\n",
        "print(\"Corpus downloaded to:\", custom_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e9d8ff5-9845-4090-8e2f-d80fa731e8d7",
      "metadata": {
        "id": "2e9d8ff5-9845-4090-8e2f-d80fa731e8d7"
      },
      "source": [
        "## Section 2: Preprocessing the Corpus Data\n",
        "\n",
        "> **Text Box:**\n",
        "> This section preprocesses the raw corpus data. It includes functions to:\n",
        "> 1. Print sample lines of the raw data file (to quickly verify its contents).\n",
        "> 2. Load and parse conversation lines from a JSONL file. Each line represents an utterance in a conversation.\n",
        "> 3. Extract sentence pairs (an input and its corresponding response) and write them to a formatted text file for subsequent training.\n",
        "> Every function is extensively commented to clarify its purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a99eeb-7e71-48aa-bcbb-25e4c777b23f",
      "metadata": {
        "id": "c7a99eeb-7e71-48aa-bcbb-25e4c777b23f"
      },
      "outputs": [],
      "source": [
        "# Function to print the first 'n' lines from a file\n",
        "def printLines(file, n=10):\n",
        "    # Open the file in binary mode\n",
        "    with open(file, 'rb') as datafile:\n",
        "        # Read all lines from the file\n",
        "        lines = datafile.readlines()\n",
        "    # Print only the first 'n' lines\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "# Define the corpus name and construct the path to the corpus\n",
        "corpus_name = \"movie-corpus\"\n",
        "corpus = os.path.join(\"/content/drive/MyDrive/Data_cornell\", corpus_name)\n",
        "\n",
        "# Display sample lines from the raw corpus file to verify its content\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "# Function to load lines and conversations from a JSONL file\n",
        "def loadLinesAndConversations(fileName):\n",
        "    # Create two empty dictionaries: one for individual lines and one for grouped conversations\n",
        "    lines = {}\n",
        "    conversations = {}\n",
        "\n",
        "    # Open the JSONL file using a specified encoding\n",
        "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
        "        for line in f:\n",
        "            # Each line in the file is parsed as a JSON object\n",
        "            lineJson = json.loads(line)\n",
        "\n",
        "            # Create a simplified line object with essential information\n",
        "            lineObj = {\n",
        "                \"lineID\": lineJson[\"id\"],\n",
        "                \"characterID\": lineJson[\"speaker\"],\n",
        "                \"text\": lineJson[\"text\"]\n",
        "            }\n",
        "\n",
        "            # Save the line object in the lines dictionary\n",
        "            lines[lineObj['lineID']] = lineObj\n",
        "\n",
        "            # For grouping: if the conversation_id is new, create a new conversation object\n",
        "            if lineJson[\"conversation_id\"] not in conversations:\n",
        "                convObj = {\n",
        "                    \"conversationID\": lineJson[\"conversation_id\"],\n",
        "                    \"movieID\": lineJson[\"meta\"][\"movie_id\"],\n",
        "                    \"lines\": [lineObj]  # Initialize with the current line\n",
        "                }\n",
        "            else:\n",
        "                # Otherwise, insert the line at the beginning of the existing conversation (for reverse chronological order)\n",
        "                convObj = conversations[lineJson[\"conversation_id\"]]\n",
        "                convObj[\"lines\"].insert(0, lineObj)\n",
        "            # Save/update the conversation object in the conversations dictionary\n",
        "            conversations[convObj[\"conversationID\"]] = convObj\n",
        "\n",
        "    # Return both dictionaries\n",
        "    return lines, conversations\n",
        "\n",
        "# Function to extract sentence pairs (input and target responses) from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    # Loop over each conversation in the dictionary\n",
        "    for conversation in conversations.values():\n",
        "        # Iterate over each pair of consecutive lines (ignores the last line as it has no reply)\n",
        "        for i in range(len(conversation[\"lines\"]) - 1):\n",
        "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "\n",
        "            # Only add the pair if both input and target lines are non-empty\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs\n",
        "\n",
        "# Process the corpus by parsing the JSONL file\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "# Define the file path to save the newly formatted data\n",
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "# Define the delimiter to use when writing the new file (a tab in this case)\n",
        "delimiter = str(codecs.decode('\\t', \"unicode_escape\"))\n",
        "\n",
        "# Write the extracted sentence pairs into a new CSV (tab delimited) file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)  # Each row represents an input-response pair\n",
        "\n",
        "# Print sample lines from the newly formatted file to verify correctness\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7d71d5-c2b2-4375-a833-5a83b3119718",
      "metadata": {
        "id": "8d7d71d5-c2b2-4375-a833-5a83b3119718"
      },
      "source": [
        "## Section 3: Vocabulary and Tokenization Setup\n",
        "\n",
        "> **Text Box:**\n",
        "> Here we define the vocabulary management and text normalization routines.\n",
        ">\n",
        "> **Special Tokens:**\n",
        "> - PAD: Padding token to make sequences the same length\n",
        "> - SOS: Start-of-Sentence token\n",
        "> - EOS: End-of-Sentence token\n",
        ">\n",
        "> **Voc Class:** Maintains mappings between words and unique indices, counts words, and can trim rarely-used words.\n",
        ">\n",
        "> **Normalization Functions:** Help process the raw text into a consistent format (ASCII, lowercase, trimmed, and cleaned of non-letter characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "662c77af-55c5-4faa-a3f1-8e09b84a38a9",
      "metadata": {
        "id": "662c77af-55c5-4faa-a3f1-8e09b84a38a9"
      },
      "outputs": [],
      "source": [
        "# Define special token constants\n",
        "PAD_token = 0  # Padding token\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "# Class to build and maintain the vocabulary\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        # Name or tag for the vocabulary instance\n",
        "        self.name = name\n",
        "        self.trimmed = False  # Flag to indicate if trimming has been done\n",
        "        self.word2index = {}  # Dictionary mapping words to indices\n",
        "        self.word2count = {}  # Dictionary counting how often each word appears\n",
        "        # Reverse mapping: indices to words, pre-populated with special tokens\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count of words; starts at 3 because of the three special tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # Split the sentence into words and add each word to the vocabulary\n",
        "        for word in sentence.split():\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # If the word is new to the vocabulary, add it\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            # Otherwise, just update the word frequency\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        # Ensure that trimming happens only once\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        # Identify words that appear at least 'min_count' times\n",
        "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
        "        print('keep_words {} / {} = {:.4f}'.format(len(keep_words), len(self.word2index), len(keep_words)/len(self.word2index)))\n",
        "\n",
        "        # Reinitialize the dictionaries to only include the frequently occurring words\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        # Add back each word that met the minimum count threshold\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "MAX_LENGTH = 10  # Maximum sentence length to consider when filtering sentence pairs\n",
        "\n",
        "# Function to convert Unicode string to ASCII\n",
        "def unicodeToAscii(s):\n",
        "    # Normalize the string into a canonical form and filter out combining characters\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "                   if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Function to normalize strings by converting to lowercase, trimming whitespace, and removing non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    # Add a space before punctuation for better tokenization\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    # Replace any character that is not a letter or punctuation with a space\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    # Remove extra spaces\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Function to read the formatted data file and return the vocabulary and sentence pairs\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read entire file as a string and split into individual lines\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    # For each line, split by tab and normalize both parts of the pair\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)  # Create a new vocabulary object\n",
        "    return voc, pairs\n",
        "\n",
        "# Filter function to keep only pairs where both sentences are shorter than MAX_LENGTH\n",
        "def filterPair(p):\n",
        "    return len(p[0].split()) < MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
        "\n",
        "# Apply the filter to all sentence pairs\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a13b32f-7994-4cfd-9d2e-dc69641071ed",
      "metadata": {
        "id": "9a13b32f-7994-4cfd-9d2e-dc69641071ed"
      },
      "source": [
        "## Section 4: Data Preparation for Model Input\n",
        "\n",
        "> **Text Box:**\n",
        "> This section defines helper functions to convert sentences to numerical indices, pad sequences with the PAD token, and create masks so the model ignores padding during training.\n",
        "> These functions ultimately generate batches of data to be fed into the model during training.\n",
        "> Each function has extensive comments to clarify its purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0693cc-6022-4bfa-a74b-7a71d5fc6818",
      "metadata": {
        "id": "2c0693cc-6022-4bfa-a74b-7a71d5fc6818"
      },
      "outputs": [],
      "source": [
        "# Convert a sentence into a list of indices using the vocabulary mapping, and append the EOS token\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split()] + [EOS_token]\n",
        "\n",
        "# Zero-pad a list of sequences so that they all have the same length\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    # itertools.zip_longest groups elements from each sequence; missing values are filled with 'fillvalue'\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "# Create a binary matrix which indicates non-PAD tokens (1 for real token, 0 for PAD)\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for seq in l:\n",
        "        # For each token in the sequence, check if it is not the padding token\n",
        "        m.append([0 if token == PAD_token else 1 for token in seq])\n",
        "    return m\n",
        "\n",
        "# Prepare input variables (padded sequences and lengths) from a list of sentences\n",
        "def inputVar(l, voc):\n",
        "    # Convert each sentence into a sequence of indices\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    # Record the original lengths of each sequence\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Pad the sequences with the PAD token so that all sequences have the same length\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    # Convert the padded list into a PyTorch LongTensor\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Prepare output variables (padded sequences, mask matrix, and max sequence length) for the target sentences\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max(len(indexes) for indexes in indexes_batch)  # Find the length of the longest sequence\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    # Create a mask where PAD tokens are marked as 0\n",
        "    mask = torch.BoolTensor(binaryMatrix(padList))\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Combine input and target sentences into a single batch, properly padded and sorted by sentence length\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    # Sort sentence pairs in descending order of the length of the input sentence\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split()), reverse=True)\n",
        "    # Separate input and target sentences into two separate lists\n",
        "    input_batch = [pair[0] for pair in pair_batch]\n",
        "    output_batch = [pair[1] for pair in pair_batch]\n",
        "    # Get padded tensor and lengths for input sentences\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    # Get padded tensor, mask, and max target sentence length for target sentences\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "# Create a small random batch to verify data preparation functionality\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "# Print out the prepared tensors to inspect the shapes and contents\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f0136d5-eab2-4a13-b5cc-3c7e0a0bc60f",
      "metadata": {
        "id": "6f0136d5-eab2-4a13-b5cc-3c7e0a0bc60f"
      },
      "source": [
        "## Section 5: Model Architecture\n",
        "\n",
        "> **Text Box:**\n",
        "> This section defines the architecture of the chatbot model:\n",
        ">\n",
        "> 1. **EncoderRNN:** A GRU-based encoder that processes the input embeddings and produces a hidden state for each token in the input.\n",
        ">\n",
        "> 2. **Attn Module:** Implements Luong-style attention. It supports three methods: 'dot', 'general', and 'concat'. The attention mechanism computes a weighted sum of encoder outputs based on the current decoder hidden state.\n",
        ">\n",
        "> 3. **LuongAttnDecoderRNN:** A decoder that uses the attention weights along with its GRU outputs to produce predictions for the next word in the sequence, one word at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76cdb2b3-95ea-4098-9a5d-a083be39d6ae",
      "metadata": {
        "id": "76cdb2b3-95ea-4098-9a5d-a083be39d6ae"
      },
      "outputs": [],
      "source": [
        "# EncoderRNN class using a bidirectional GRU\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        # Number of layers for the GRU\n",
        "        self.n_layers = n_layers\n",
        "        # Hidden size for the GRU\n",
        "        self.hidden_size = hidden_size\n",
        "        # Embedding layer to convert word indices into embeddings\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Define the bidirectional GRU. Note that input_size and hidden_size are both hidden_size\n",
        "        # because the input is expected to be already embedded\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert input indices to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "\n",
        "        # Pack the embedded sequences for efficient processing in the GRU\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "\n",
        "        # Pass the packed sequence through the GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        # Unpack the sequences back to padded format\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "\n",
        "        # Sum the outputs from both directions (forward and backward)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "# Attention module implementing Luong attention\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        # Ensure the provided attention method is one of the allowed choices\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(f\"{self.method} is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # For 'general' attention, we use a linear layer to transform the encoder output\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "        # For 'concat' attention, we concatenate and then transform; also define a parameter vector v\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        # Element-wise multiplication and sum the result along the feature dimension\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        # Transform the encoder output and then compute the dot product\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        # Concatenate the hidden state and encoder output, then apply a nonlinear transformation\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), dim=2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Choose the appropriate scoring function based on the specified method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose dimensions so that the batch is the first dimension\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Normalize the attention scores to probabilities\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "# Decoder RNN that integrates Luong attention\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "        # Save the model configuration\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define the embedding layer (shared with the encoder)\n",
        "        self.embedding = embedding\n",
        "        # Apply dropout to embeddings to prevent overfitting\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Define the GRU layer\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "\n",
        "        # Linear layer to combine GRU outputs with attention context\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        # Final output layer mapping to vocabulary size\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Convert current input token to its embedding\n",
        "        embedded = self.embedding(input_step)\n",
        "        # Apply dropout to the embedding\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "\n",
        "        # Forward pass through the GRU; note that the decoder is unidirectional\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "\n",
        "        # Compute attention weights using the current GRU output and the encoder outputs\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "\n",
        "        # Compute context vector as the weighted sum of encoder outputs\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "\n",
        "        # Remove the time-step dimension from GRU output and context\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "\n",
        "        # Concatenate the GRU output and the context vector\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        # Pass through a tanh activation after linear combination to produce the final representation\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "\n",
        "        # Predict the next word using a softmax layer\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c13b1b-c061-40ac-a3cc-f1a5d5667ec4",
      "metadata": {
        "id": "d2c13b1b-c061-40ac-a3cc-f1a5d5667ec4"
      },
      "source": [
        "## Section 6: Training Functions\n",
        "\n",
        "> **Text Box:**\n",
        "> This section defines the training loop and associated functions:\n",
        ">\n",
        "> - **maskNLLLoss:** Computes the negative log-likelihood loss while ignoring padded elements.\n",
        "> - **train:** Runs a single training iteration, including forward propagation, loss computation, backpropagation, and optimizer stepping.\n",
        "> - **trainIters:** Manages multiple iterations of training and handles checkpoint saving and progress printing.\n",
        ">\n",
        "> Each function includes detailed comments explaining each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c928a65a-3e4c-4ab4-9ac6-29244ea6ef7a",
      "metadata": {
        "id": "c928a65a-3e4c-4ab4-9ac6-29244ea6ef7a"
      },
      "outputs": [],
      "source": [
        "# Compute the loss while taking into account only non-PAD tokens\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    # Total number of non-PAD tokens in this batch\n",
        "    nTotal = mask.sum()\n",
        "    # Calculate the negative log-likelihood loss for the predicted probabilities corresponding to the target tokens\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    # Apply the mask to remove padded elements and compute the mean loss\n",
        "    loss = crossEntropy.masked_select(mask).mean().to(device)\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "# Single training iteration over one batch\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "    # Zero the gradients for both encoder and decoder optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Move input, target, and mask tensors to the correct device (GPU or CPU)\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Ensure lengths are on the CPU for packing\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Run the encoder forward pass; it outputs features and hidden states\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Prepare the first input of the decoder which is the SOS token for each sentence in the batch\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]).to(device)\n",
        "\n",
        "    # Initialize decoder hidden state with encoder's final hidden state (for the first 'n_layers' layers)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Decide whether to use teacher forcing based on a random number\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Run decoder forward pass one time-step at a time\n",
        "    if use_teacher_forcing:\n",
        "        # For each time step, feed the correct token from the target as the next input\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Next input is the actual target token\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Compute the loss for this time step\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        # Without teacher forcing, use the decoder's own predictions as the next input\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Get the most likely word token from the output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            # Prepare the token to be fed into the next time step\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]).to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Backpropagate the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients to prevent exploding gradients\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Update model parameters\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "# Function to run multiple training iterations\n",
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n",
        "               encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip,\n",
        "               corpus_name, teacher_forcing_ratio, profile=False, save_model=False):\n",
        "    # Create training batches for each iteration using randomly selected sentence pairs\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1  # Default starting iteration\n",
        "    print_loss = 0\n",
        "\n",
        "    # If resuming from a checkpoint, update start_iteration (this part can be extended)\n",
        "    print(\"Training...\")\n",
        "\n",
        "    # Iterate over each training batch\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Train over the current batch and get the loss\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress every 'print_every' iterations\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(\n",
        "                iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save a checkpoint every 'save_every' iterations\n",
        "        if iteration % save_every == 0:\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(\n",
        "                encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f400f06-37f8-4969-9d29-c85564fa4675",
      "metadata": {
        "id": "8f400f06-37f8-4969-9d29-c85564fa4675"
      },
      "source": [
        "## Section 7: Evaluation and Interaction\n",
        "\n",
        "> **Text Box:**\n",
        "> In this section we define functions for model inference:\n",
        ">\n",
        "> - **GreedySearchDecoder:** Uses a greedy approach to select the most likely token at each time step.\n",
        "> - **evaluate:** Processes an input sentence and converts model outputs back to words.\n",
        "> - **evaluateInput:** Provides an interactive loop for chatting with the bot.\n",
        "> Extensive inline comments explain each step of the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dba2e43f-33fa-49e5-88e4-8dd296689f37",
      "metadata": {
        "id": "dba2e43f-33fa-49e5-88e4-8dd296689f37"
      },
      "outputs": [],
      "source": [
        "# Greedy decoder that uses the encoder and decoder to generate responses one token at a time\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder  # The pre-trained encoder model\n",
        "        self.decoder = decoder  # The pre-trained decoder model\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Run the encoder to get outputs and hidden states\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # Use the final hidden state of the encoder as the initial hidden state for the decoder\n",
        "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "\n",
        "        # Initialize the decoder input with the SOS token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "\n",
        "        # Containers for the output tokens and scores\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "\n",
        "        # Decode one token at a time up to max_length\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Choose the token with the highest probability\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Append the chosen token and its score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare the token for the next iteration (add a time-step dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        return all_tokens, all_scores\n",
        "\n",
        "# Evaluate an input sentence and return the decoded words\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    # Convert the sentence into indices using the vocabulary\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Calculate the lengths of the sentence(s)\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Create a tensor representing the input batch (transpose for correct dimensions)\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1).to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Use the searcher (decoder) to generate output tokens and associated scores\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    # Convert the output token indices back into words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "# Interactive loop to chat with the bot\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    while True:\n",
        "        try:\n",
        "            # Read user input\n",
        "            input_sentence = input('> ')\n",
        "            if input_sentence.lower() in ['q', 'quit']:\n",
        "                break  # Exit the loop if the user types 'q' or 'quit'\n",
        "            # Normalize the input sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Generate the output words from the model\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Remove special tokens from the output\n",
        "            output_words = [x for x in output_words if x not in ['EOS', 'PAD']]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            # In case an unknown word is encountered, print an error message\n",
        "            print(\"Error: Encountered unknown word.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0182513e-8a6a-402f-96bd-96b7c36e9b84",
      "metadata": {
        "id": "0182513e-8a6a-402f-96bd-96b7c36e9b84"
      },
      "source": [
        "## Section 8: Model Initialization, Checkpoint Loading, and Training Setup\n",
        "\n",
        "> **Text Box:**\n",
        "> In this final section we configure model parameters (hidden sizes, number of layers, dropout rates, etc.),\n",
        "> initialize the embedding layer, encoder, and decoder, and finally set up the training process.\n",
        "> A wrapper function is provided to encapsulate training initialization (with optional checkpoint support and Wandb logging)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c97d25c-7de2-46a6-aa15-4e4ef2d1a0d8",
      "metadata": {
        "id": "7c97d25c-7de2-46a6-aa15-4e4ef2d1a0d8"
      },
      "outputs": [],
      "source": [
        "# Configuration parameters for the model\n",
        "model_name = 'cb_model'        # Name or identifier for the model\n",
        "attn_model = 'dot'             # Attention mechanism type to use ('dot', 'general', or 'concat')\n",
        "hidden_size = 512              # Dimensionality of the hidden state and embeddings\n",
        "encoder_n_layers = 2           # Number of layers in the encoder\n",
        "decoder_n_layers = 2           # Number of layers in the decoder\n",
        "dropout = 0.1                  # Dropout rate to avoid overfitting\n",
        "batch_size = 64                # Size of each training batch\n",
        "\n",
        "loadFilename = None            # Specify a checkpoint file to load (None to start from scratch)\n",
        "checkpoint_iter = 4000         # Iteration number of the checkpoint (if applicable)\n",
        "\n",
        "# Wrapper function to initialize and start training\n",
        "def wrapper_train(config=None, profile=False, save_model=False):\n",
        "    import wandb  # For logging training metrics (Weights & Biases)\n",
        "    run = wandb.init(project=\"W&BProjectName\")\n",
        "    config = run.config if config is None else config\n",
        "\n",
        "    # Set local configuration parameters; these could be overridden by 'config'\n",
        "    model_name = 'cb_model'\n",
        "    attn_model = 'dot'\n",
        "    hidden_size = 500\n",
        "    encoder_n_layers = 2\n",
        "    decoder_n_layers = 2\n",
        "    dropout = 0.1\n",
        "    batch_size = 64\n",
        "\n",
        "    print('Building encoder and decoder ...')\n",
        "\n",
        "    # Initialize the embedding layer with the vocabulary size and the chosen hidden size\n",
        "    embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "\n",
        "    # Initialize the encoder using the embedding layer\n",
        "    encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "\n",
        "    # Initialize the decoder with attention, using the same embedding layer\n",
        "    decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "\n",
        "    # Move both models to the selected device (GPU or CPU)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    print('Models built and ready to go!')\n",
        "\n",
        "    # Retrieve training hyperparameters from the configuration\n",
        "    clip = config.clip\n",
        "    teacher_forcing_ratio = config.tf_ratio\n",
        "    learning_rate = config.lr\n",
        "    decoder_learning_ratio = config.decoder_lrn_ratio\n",
        "    n_iteration = 4000\n",
        "    print_every = 1\n",
        "    save_every = 500\n",
        "\n",
        "    # Ensure models are in training mode (this activates dropout layers, etc.)\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    print('Building optimizers ...')\n",
        "    # Choose an optimizer (Adam or SGD) based on the configuration\n",
        "    optimizer_fn = optim.Adam if config.optimizer == \"adam\" else optim.SGD\n",
        "    encoder_optimizer = optimizer_fn(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optimizer_fn(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "    # Move optimizer state tensors to the correct device\n",
        "    for state in encoder_optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)\n",
        "\n",
        "    for state in decoder_optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)\n",
        "\n",
        "    print(\"Starting Training!\")\n",
        "\n",
        "    # Begin training iterations\n",
        "    trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "               print_every, save_every, clip, corpus_name, config.tf_ratio, profile, save_model)\n",
        "    return encoder, decoder\n",
        "\n",
        "# Define a configuration dataclass for clearer parameter management\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    clip: float              # Gradient clipping value\n",
        "    tf_ratio: float          # Teacher forcing ratio (probability of using true target as next input)\n",
        "    lr: float                # Learning rate\n",
        "    optimizer: str           # Optimizer type (e.g., 'adam' or 'sgd')\n",
        "    decoder_lrn_ratio: float # Multiplier for decoder's learning rate relative to encoder's\n",
        "\n",
        "# Start training using the configuration defined above\n",
        "encoder, decoder = wrapper_train(Config(clip=0.0, tf_ratio=0.0, lr=0.0001, optimizer=\"adam\", decoder_lrn_ratio=1.0), profile=True, save_model=True)\n",
        "\n",
        "# Set additional training parameters (could also be part of the Config class)\n",
        "clip = 100\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure the models are still in training mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# Move optimizer states to the selected device\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(device)\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.to(device)\n",
        "\n",
        "print(\"Training iteration in progress...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}